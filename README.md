# Humanâ€“Machine Difficulty Kit

Open-source code release for our forthcoming paper on Humanâ€“Machine Difficulty Alignment in educational QA.
This repo provides a complete, reproducible pipeline to: (1) prepare RACE & Eedi data, (2) score MCQs with LLMs, (3) compute Data Maps and calibration (ECE / temperature scaling), and (4) quantify alignment between human-perceived and model-perceived difficulty.

â¸»

âœ¨ TL;DR
	â€¢	Goal: Measure how well model-perceived difficulty (confidence, variability, NLL) aligns with human difficulty (error rate, wrong-with-confidence, IRT-ready signals).
	â€¢	Datasets: RACE (reading comprehension) and Eedi NeurIPS 2020 Task 3/4 (K12 multiple-choice with images).
	â€¢	Models: Plug any API model (OpenAI/Qwen/DeepSeek/â€¦); a dummy client is included to validate the pipeline.
	â€¢	Methods:
	â€¢	Scoring uses per-option probabilities p(Aâ€¦D) with temperature=0 (no sampling).
	â€¢	Data Map from per-item mean/std of p(\text{correct}) across rounds.
	â€¢	Calibration (ECE, Brier, temperature scaling).
	â€¢	Alignment via rank-correlation & contingency analysis against human difficulty.
	â€¢	Repro first: Everything runs from a single CLI: runner.py.

â¸»

ğŸ—‚ Repository Layout

human_machine_difficulty_kit/
â”œâ”€ README.md
â”œâ”€ requirements.txt
â”œâ”€ config.example.yaml
â”œâ”€ runner.py
â”œâ”€ sample_data/
â”‚  â”œâ”€ race_demo.jsonl
â”‚  â””â”€ eedi_interactions_demo.csv
â”œâ”€ src/
â”‚  â”œâ”€ utils/
â”‚  â”‚  â””â”€ io.py
â”‚  â”œâ”€ data/
â”‚  â”‚  â”œâ”€ prepare_race.py
â”‚  â”‚  â””â”€ prepare_eedi.py
â”‚  â”œâ”€ scoring/
â”‚  â”‚  â”œâ”€ interface.py
â”‚  â”‚  â”œâ”€ prompts.py
â”‚  â”‚  â”œâ”€ dummy_client.py
â”‚  â”‚  â””â”€ openai_client_stub.py
â”‚  â””â”€ analysis/
â”‚     â”œâ”€ calibration.py
â”‚     â”œâ”€ datamap.py
â”‚     â”œâ”€ alignment.py
â”‚     â””â”€ tables_figures.py


â¸»

ğŸ”§ Installation

git clone <your-repo-url>
cd human_machine_difficulty_kit
python -m venv .venv && source .venv/bin/activate   # on Windows: .venv\Scripts\activate
pip install -r requirements.txt

Optional (for RACE auto-download):

pip install datasets


â¸»

ğŸš€ Quick Start (Demo without any external data)

Validate the entire pipeline with a tiny synthetic set and a random scorer:

python runner.py demo --rounds 3

Outputs (check outputs/demo/):
	â€¢	scores.csv (per-round p(correct) & correctness),
	â€¢	datamap.csv (mean/std per item).

â¸»

ğŸ“¦ Data Preparation

RACE (ReAding Comprehension)

Use the Hugging Face dataset (recommended):

python -m src.data.prepare_race --split test
# writes: data/race/processed/race_test.jsonl

Each record has:

{
  "id": "passageId#Qk",
  "passage": "...",
  "question": "...",
  "options": {"A":"...","B":"...","C":"...","D":"..."},
  "answer": "A|B|C|D"
}

Eedi NeurIPS 2020 (Task 3/4)

Download the official CSVs per license and place them at:

data/eedi/raw/
  interactions_task34.csv   # must contain: question_id, IsCorrect, AnswerValue, CorrectAnswer, Confidence
  questions_task34.csv      # optional: stem/options/image_path for richer prompts

Preprocess:

python -m src.data.prepare_eedi \
  --in data/eedi/raw/interactions_task34.csv \
  --out data/eedi/processed \
  [--questions data/eedi/raw/questions_task34.csv]

This produces:
	â€¢	question_summary_task34.csv with human-side signals:
	â€¢	error_rate = 1 - mean(IsCorrect)
	â€¢	wrong_conf = mean((1 - IsCorrect) * Confidence/100)  â† emphasizes â€œwrong with confidenceâ€
	â€¢	diff_conf_old (legacy kept for reference)
	â€¢	option_entropy (student choice dispersion)
	â€¢	human_label_v2 via quantiles over wrong_conf (simpleH|mediumH|hardH)
	â€¢	task34_for_llm.jsonl unified MCQ list with optional image_path.

Note: For Eedi, many questions rely on the image. Use a vision-capable model and pass the image during scoring to ensure humanâ€“model comparability.

â¸»

ğŸ¤– Scoring Models (Plug-in Interface)

Implement your model in src/scoring/openai_client_stub.py or add new clients.
Contract (src/scoring/interface.py):
	â€¢	Input: prompt + options.
	â€¢	Output: per-option probabilities p(A..D) (normalized), and the chosen label.

Critical requirements (to avoid common pitfalls):
	â€¢	Compute per-option scores and softmax across A/B/C/D.
Do NOT use â€œfirst generated token probabilityâ€ as confidence.
	â€¢	Disable sampling (temperature=0) for scoring.
	â€¢	For multimodal items (Eedi), make sure to include the image in the scoring request.

Environment variables (as needed):

OPENAI_API_KEY=...
DASHSCOPE_API_KEY=...      # Qwen
DEEPSEEK_API_KEY=...       # DeepSeek


â¸»

ğŸ§ª Running Experiments

1) Score a dataset

# Example: RACE test with your model, 5 rounds for Data Map
python runner.py score --dataset race --split test \
  --model openai:gpt-4o --rounds 5 --out outputs/race_gpt4o

This writes outputs/<run>/scores.csv with:
	â€¢	question_id, run_id, chosen, p_correct, correct

2) Build a Data Map

python runner.py datamap \
  --inp outputs/race_gpt4o/scores.csv \
  --out outputs/race_gpt4o \
  --quantile

	â€¢	Aggregates to per-item mean_p, std_p, acc, n.
	â€¢	Assigns regions (easy|ambiguous|hard|impossible) using adaptive quantiles (or fixed thresholds if you omit --quantile).

3) Calibrate Confidence (ECE / Temperature Scaling)

python runner.py calibrate \
  --inp outputs/race_gpt4o/scores.csv \
  --out outputs/race_gpt4o

Outputs calibration.json with:
	â€¢	ece_before/after, brier_before/after, temperature T, NLL changes.

Use calibrated probabilities when comparing thresholds across models (to avoid unfair comparisons due to calibration drift).

4) Humanâ€“Machine Alignment (Eedi)

python runner.py align \
  --scores outputs/eedi_gpt4o/scores.csv \
  --human  data/eedi/processed/question_summary_task34.csv \
  --out    outputs/eedi_gpt4o

	â€¢	Produces merged table and reports Spearman/Kendall between human difficulty (wrong_conf) and model difficulty (1 - mean_p(correct)).

â¸»

ğŸ“ˆ Metrics & Definitions
	â€¢	Model-side
	â€¢	p(correct): probability assigned to the gold option.
	â€¢	Data Map: per-item mean_p and std_p across rounds; regioning via thresholds or quantiles.
	â€¢	ECE (bin=10) and Brier score; optional temperature scaling.
	â€¢	Stability: number of unique answers across rounds (not exposed by default; easy to add).
	â€¢	Human-side (Eedi)
	â€¢	error_rate = 1 - mean(IsCorrect)
	â€¢	wrong_conf = mean((1 - IsCorrect) * Confidence/100) â† captures â€œconfidently wrongâ€
	â€¢	option_entropy across student choices
	â€¢	human_label_v2 via quantiles over wrong_conf (simpleH|mediumH|hardH)

We keep diff_conf_old = 1 - mean(IsCorrect * Confidence/100) for completeness, but analyses should prefer wrong_conf when the goal is to weight confidence in mistakes.

â¸»

ğŸ“¤ Typical Outputs
	â€¢	scores.csv â€” per round predictions and p_correct
	â€¢	datamap.csv â€” per item mean_p, std_p, acc, region
	â€¢	calibration.json â€” ECE/Brier before/after, temperature T
	â€¢	human_machine_merge.csv â€” for correlation analyses (Eedi)

You can add plots via src/analysis/tables_figures.py (reliability diagrams, Data Map scatter, heatmaps).

â¸»

ğŸ§­ Best Practices & Pitfalls (Read before you run!)
	â€¢	Donâ€™t use first-token logprob as â€œconfidenceâ€. Always compute per-option probabilities and normalize across Aâ€“D.
	â€¢	Turn off sampling for scoring (temperature=0). Use multiple rounds only when you want variability estimates (e.g., Data Map).
	â€¢	Calibrate before comparing models with a fixed threshold (e.g., Ï„=0.8). Alternatively, compare at model-specific quantiles to equalize support.
	â€¢	Feed images for Eedi items that require them; text-only inputs are not comparable to human performance.
	â€¢	Thresholds for Data Map regions are data- and model-dependent; prefer quantile-based or clustering approaches for robust partitions.

â¸»

ğŸ›  Extending / Replacing Models
	â€¢	Implement ModelClient.score_mcq(prompt, options) â†’ {probs, chosen}.
	â€¢	For OpenAI-like APIs, two robust scoring patterns:
	1.	Per-option continuation scoring (average token log-likelihood over the option text).
	2.	Label-only next-token scoring (restrict logits to A|B|C|D via logit bias and normalize).
	â€¢	Add your client under src/scoring/ and switch with --model your_client:your_model_name.

â¸»

ğŸ” Reproducibility Checklist
	â€¢	Random seeds specified in clients and sampling (if used).
	â€¢	Rounds --rounds N documented for each run.
	â€¢	Calibration split (80/20) fixed in calibrate.
	â€¢	Dataset splits fixed (RACE test or your chosen split).
	â€¢	All scripts produce versioned outputs under outputs/<run>/.

â¸»

ğŸ“œ License & Data Terms
	â€¢	Code is released under MIT License (see LICENSE if included).
	â€¢	Datasets (RACE, Eedi) are subject to their original licenses; please obtain and use them accordingly.
	â€¢	Respect privacy and data-use restrictions for any student interaction logs.
