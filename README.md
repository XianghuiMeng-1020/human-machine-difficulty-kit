# ğŸ§  Humanâ€“Machine Difficulty Kit

[![Python](https://img.shields.io/badge/Python-3.10%2B-blue)](https://www.python.org/)
[![NumPy](https://img.shields.io/badge/NumPy-%E2%89%A52.0-013243)](https://numpy.org/)
[![pandas](https://img.shields.io/badge/pandas-%E2%89%A52.0-150458)](https://pandas.pydata.org/)
[![scikit--learn](https://img.shields.io/badge/scikit--learn-1.7%2B-F7931E)](https://scikit-learn.org/)
[![StatsModels](https://img.shields.io/badge/StatsModels-0.14%2B-005F87)](https://www.statsmodels.org/)
[![License](https://img.shields.io/badge/License-MIT-purple)](LICENSE)

> A reproducible toolkit for analyzing and aligning **human** vs **machine** difficulty across education datasets (EEDI, RACE, EDNet/KT1, and Synthetic-200).

---

## âœ¨ Features

- **Unified pipelines** for EEDI, RACE, EDNet/KT1 and Synthetic-200 with consistent CSV outputs  
- **Alignment metrics**: Spearman Ï, error/confidence gaps, calibration, riskâ€“coverage  
- **Statistical modeling**: GLM/GEE, mixed effects, tag-wise analysis, temperature scaling  
- **Model-agnostic scoring API**: swap in any model via `code/src/scoring/interface.py`  
- **Paper-ready tables** under `analysis/` and `paper_assets/` (figures optional)

---

## ğŸš€ Quick Start

### Prerequisites
- Python **3.10+** (tested up to 3.13)
- macOS/Linux shell environment
- Recommended: create and activate a virtualenv

### Installation

```bash
git clone https://github.com/XianghuiMeng-1020/human-machine-difficulty-kit.git
cd human-machine-difficulty-kit

python -m venv .venv && source .venv/bin/activate   # optional but recommended
pip install -e .
# optional extras used by dataset scripts / plotting
pip install -r code/requirements.txt
```

### Minimal Repro (tables only)

```bash
# Seed a global alignment table from Synthetic-200 (no external API keys needed)
python code/experiments_20251029/baselines/register_synthetic_divergence.py

# Aggregate global + paper-ready tables
python code/experiments_20251029/make_global_alignment_table.py
python code/experiments_20251029/make_paper_global_table.py
```

**Expected outputs**

```
analysis/global/global_alignment_table.csv
paper_assets/RESULTS_GLOBAL_BASELINES.csv
```

For EEDI / RACE / EDNet pipelines, see **Development â†’ Typical Flow** to populate additional tables under `analysis/`.

---

## ğŸ§© Core Components

- **Alignment & Metrics** â€” Spearman Ï (item-level), error/confidence gaps, per-tag aggregates  
- **Calibration & Selective Prediction** â€” Temperature scaling, reliability summaries, riskâ€“coverage curves  
- **Model-Agnostic Scoring** â€” Standard interface to plug in any model backend via `scoring.interface.Scorer`

---

## ğŸ”§ Technical Details

- **Metrics**: Spearman Ï, error/confidence gap, calibration AUC/CI, tag-level GLM/GEE  
- **Stats**: generalized linear models, mixed effects, robust SE  
- **Stack**: NumPy, pandas, SciPy, StatsModels, scikit-learn (Matplotlib/Seaborn optional)  
- **Design**: CSV-first; plots are optional and can be regenerated from tables

---

## ğŸ“ Project Structure

```
human-machine-difficulty-kit/
â”œâ”€â”€ .github/
â”œâ”€â”€ code/
        â”‚   â”œâ”€â”€ EDNET_ANSWER_KEY_STATUS.md
        â”‚   â”œâ”€â”€ FIGLIST.md
        â”‚   â”œâ”€â”€ LEAKAGE_AUDIT.md
        â”‚   â”œâ”€â”€ OVERLAP_CASE_STUDIES.md
    â”œâ”€â”€ experiments_20251029/
        â”‚       â”‚   â”‚   â”œâ”€â”€ cl4kt_diff/
        â”‚       â”‚   â”‚   â”œâ”€â”€ register_all_baselines.py
        â”‚       â”‚   â”‚   â”œâ”€â”€ register_synthetic_divergence.py
        â”‚   â”œâ”€â”€ ednet_majority/
        â”‚       â”‚   â”‚   â”œâ”€â”€ 01_pick_majority_answer.py
        â”‚       â”‚   â”‚   â”œâ”€â”€ 02_apply_pseudo_contents.py
        â”‚   â”œâ”€â”€ paper_assets/
        â”‚       â”‚   â”‚   â”œâ”€â”€ eedi_tau08/
        â”‚       â”‚   â”‚   â”œâ”€â”€ mv-hmda/
        â”‚       â”‚   â”‚   â”œâ”€â”€ race/
        â”‚       â”‚   â”‚   â”œâ”€â”€ synthetic_real/
        â”‚       â”‚   â”‚   â”œâ”€â”€ BASELINES_REPRO_NOTES.md
        â”‚       â”‚   â”‚   â”œâ”€â”€ EXPERIMENTS_INDEX.md
        â”‚       â”‚   â”‚   â”œâ”€â”€ README.md
        â”‚       â”‚   â”‚   â”œâ”€â”€ SHA256SUMS.txt
        â”‚   â”œâ”€â”€ scripts/
        â”‚       â”‚   â”‚   â”œâ”€â”€ 00_build_tidy_from_raw.py
        â”‚       â”‚   â”‚   â”œâ”€â”€ 01_continuous_alignment_and_logit.py
        â”‚   â”œâ”€â”€ synthetic/
        â”‚       â”‚   â”‚   â”œâ”€â”€ gen_questions_200.jsonl
        â”‚       â”‚   â”‚   â”œâ”€â”€ make_200_questions.py
        â”‚       â”‚   â”‚   â”œâ”€â”€ summarize_divergence_by_topic.py
        â”‚   â”œâ”€â”€ SHA256SUMS.txt
        â”‚   â”œâ”€â”€ align_index.py
        â”‚   â”œâ”€â”€ align_index_labeled.py
        â”‚   â”œâ”€â”€ analyze_divergence_full.py
        â”‚   â”œâ”€â”€ analyze_synthetic_divergence.py
        â”‚   â”œâ”€â”€ apply_alignment_head.py
        â”‚   â”œâ”€â”€ apply_alignment_head_pair.py
        â”‚   â”œâ”€â”€ apply_alignment_head_race_from_eedi.py
        â”‚   â”œâ”€â”€ apply_joint_head_to_all.py
        â”‚   â”œâ”€â”€ apply_race_head_to_eedi.py
        â”‚   â”œâ”€â”€ compare_models.py
        â”‚   â”œâ”€â”€ csv_to_markdown.py
        â”‚   â”œâ”€â”€ ednet_compare_two_samples.py
        â”‚   â”œâ”€â”€ ednet_flatten_any.py
        â”‚   â”œâ”€â”€ ednet_flatten_from_dir_uid.py
        â”‚   â”œâ”€â”€ ednet_flatten_from_dir_uid_fixed.py
        â”‚   â”œâ”€â”€ ednet_flatten_kt1_csv.py
        â”‚   â”œâ”€â”€ ednet_flatten_kt1_csv_small.py
        â”‚   â”œâ”€â”€ ednet_label_covaware.py
        â”‚   â”œâ”€â”€ ednet_make_balanced_subset.py
        â”‚   â”œâ”€â”€ ednet_make_proxy_labels.py
        â”‚   â”œâ”€â”€ ednet_make_proxy_labels_big.py
        â”‚   â”œâ”€â”€ ednet_make_proxy_labels_covaware.py
        â”‚   â”œâ”€â”€ ednet_make_scaling_table.py
        â”‚   â”œâ”€â”€ ednet_plot_scaling.py
        â”‚   â”œâ”€â”€ ednet_scale_run.sh
        â”‚   â”œâ”€â”€ ednet_summarize_full_proxy.py
        â”‚   â”œâ”€â”€ eedi_alignment_baselines.py
        â”‚   â”œâ”€â”€ eedi_batch_analyze.py
        â”‚   â”œâ”€â”€ eedi_behavior_descriptive.py
        â”‚   â”œâ”€â”€ eedi_behavior_regression.py
        â”‚   â”œâ”€â”€ eedi_calibration_ablation.py
        â”‚   â”œâ”€â”€ eedi_extract_text_features.py
        â”‚   â”œâ”€â”€ eedi_inspect_crosstab.py
        â”‚   â”œâ”€â”€ eedi_make_proxy_labels.py
        â”‚   â”œâ”€â”€ eedi_merge_proxy_two_models.py
        â”‚   â”œâ”€â”€ eedi_proxy_vs_model.py
        â”‚   â”œâ”€â”€ eedi_true_alignment_autonorm.py
        â”‚   â”œâ”€â”€ eedi_true_alignment_from_csv.py
        â”‚   â”œâ”€â”€ eedi_true_alignment_mapped.py
        â”‚   â”œâ”€â”€ ... (+42 more files)
    â”œâ”€â”€ release_20251028/
    â”œâ”€â”€ reports/
    â”œâ”€â”€ scripts/
        â”‚   â”œâ”€â”€ 00_build_from_filelist.py
        â”‚   â”œâ”€â”€ 00_build_from_filelist.py.bak
        â”‚   â”œâ”€â”€ 01_continuous_alignment_and_logit.py
        â”‚   â”œâ”€â”€ 01_continuous_alignment_and_logit.py.bak
        â”‚   â”œâ”€â”€ 02_misalignment_and_tau.py
        â”‚   â”œâ”€â”€ 02b_misalignment_significance.py
        â”‚   â”œâ”€â”€ 03_build_eedi_per_question_from_processed.py
        â”‚   â”œâ”€â”€ 03_calibration_auc_and_ci.py
        â”‚   â”œâ”€â”€ 04_by_cogtag_glm.py
        â”‚   â”œâ”€â”€ 04_race_stage3_reports.py
        â”‚   â”œâ”€â”€ 05_temp_scaling.py
        â”‚   â”œâ”€â”€ 05b_finalize_model_summary.py
        â”‚   â”œâ”€â”€ 06_partial_alignment_control.py
        â”‚   â”œâ”€â”€ 07_eedi_end_to_end.py
        â”‚   â”œâ”€â”€ 08_gee_mixed_effects.py
        â”‚   â”œâ”€â”€ 09_collect_artifacts.py
        â”‚   â”œâ”€â”€ 10_generalization_gap.py
        â”‚   â”œâ”€â”€ 10_generalization_gap.py.bak
        â”‚   â”œâ”€â”€ 11_eedi_mc_infer_hf.py
        â”‚   â”œâ”€â”€ race_attach_diff.py
        â”‚   â”œâ”€â”€ report_race.py
        â”‚   â”œâ”€â”€ report_race.py.bak
        â”‚   â”œâ”€â”€ sweep_race.sh
    â”œâ”€â”€ src/
        â”‚       â”‚   â”‚   â”œâ”€â”€ interface.py
        â”‚       â”‚   â”‚   â”œâ”€â”€ openai_client_stub.py
        â”‚       â”‚   â”‚   â”œâ”€â”€ prompts.py
        â”‚   â”œâ”€â”€ utils/
    â”œâ”€â”€ Makefile
    â”œâ”€â”€ filelist.txt
    â”œâ”€â”€ higher
    â”œâ”€â”€ pred_candidates.txt
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ run_4o_grid.sh
    â”œâ”€â”€ run_all.sh
    â”œâ”€â”€ runner.py
â”œâ”€â”€ docs/
    â”œâ”€â”€ EDNET_ANSWER_KEY_STATUS.md
    â”œâ”€â”€ FIGLIST.md
    â”œâ”€â”€ LEAKAGE_AUDIT.md
    â”œâ”€â”€ OVERLAP_CASE_STUDIES.md
â”œâ”€â”€ src/
â”œâ”€â”€ tests/
â””â”€â”€ README.md
```

---

## ğŸ¯ Use Cases

- **Educational Research** â€” quantify humanâ€“machine difficulty alignment  
- **Benchmark Diagnostics** â€” reveal divergence & cognitive gaps beyond accuracy  
- **Model Evaluation** â€” compare systems via alignment and riskâ€“coverage trade-offs  
- **Ablation Studies** â€” assess calibration, scaling, and tag-level effects

---

## ğŸ› ï¸ Development

### Data Layout (example)

```
data/
â”œâ”€â”€ eedi/      # per-item/per-student CSV/JSONL
â”œâ”€â”€ race/      # RACE JSON/CSV with options & keys
â”œâ”€â”€ ednet/     # KT1 flattened logs/aggregates
â””â”€â”€ synthetic/ # Synthetic-200 items/splits
```

### Typical Flow

```bash
# 1) Build dataset-level alignment tables
python code/experiments_20251029/eedi_true_alignment_from_csv.py
python code/experiments_20251029/race_alignment_from_proxy.py
python code/experiments_20251029/update_global_with_ednet_full.py

# 2) Aggregate to global + paper tables
python code/experiments_20251029/make_global_alignment_table.py
python code/experiments_20251029/make_paper_global_table.py
```

### Scoring Interface

- Implement `class YourScorer(Scorer)` in `code/src/scoring/` and point dataset scripts to use it.  
- `dummy_client.py` and an OpenAI-style `openai_client_stub.py` are included as examples.

### Testing

```bash
python -m pytest -q
python tests/test_imports.py
```

---

## ğŸ“„ License

Released under the MIT License. See [LICENSE](LICENSE).

---

## ğŸ“ Citation

`CITATION.cff` is included for GitHub-native citation. Example BibTeX:

```bibtex
@software{HMDKit_v0_1_0,
  title   = {Humanâ€“Machine Difficulty Kit (HMDKit)},
  author  = {Xianghui Meng},
  year    = {2025},
  version = {v0.1.0},
  url     = {https://github.com/XianghuiMeng-1020/human-machine-difficulty-kit},
  note    = {Open-source toolkit for humanâ€“machine difficulty alignment}
}
```

---

## ğŸ“ Contact

- **Maintainer**: Xianghui Meng â€” <xmeng19@illinois.edu>  
- **Issues**: use GitHub Issues in this repository

---

## â­ Acknowledgement

If this toolkit helps your research, please consider starring the repo!
